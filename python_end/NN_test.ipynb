{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "from utils import *\n",
    "\n",
    "class model():\n",
    "\n",
    "\tdef __init__(self,layers,dims,init_type,activation_tup):\n",
    "\t\t'''layers :means the number of layer in the network\n",
    "\t\t\tdims  :length of each layer (including the input layer)\n",
    "\t\t'''\n",
    "\t\tself.layers=layers #excluding the input layer as by convention\n",
    "\t\tself.dims=dims\n",
    "\t\tself.parameters=self.initialize_parameters(dims,init_type)\n",
    "\t\tself.init_type=init_type\n",
    "\t\tself.cache={}\n",
    "\t\tself.cost=0\n",
    "\t\tself.activation_tup=activation_tup\n",
    "\t\tself.grads={}\n",
    "\n",
    "\tdef initialize_parameters(self,dims,init_type):\n",
    "\t\t''' INPUT \t: dims=dimension of each layer list or tuple\n",
    "\t\t\tOUTPUT  : parameters Dictionary\n",
    "\t\t'''\n",
    "\t\tparameters={}\n",
    "\t\tnum_layers=self.layers\n",
    "\n",
    "\t\tfor l in range(num_layers):\n",
    "\t\t\t#He works best with relu activations in the layers(Why?)\n",
    "\t\t\tif(init_type==\"HE\"):\n",
    "\t\t\t\tparameters[\"W\"+str(l+1)]=np.random.randn(dims[l+1],dims[l])*np.sqrt(2/dims[l])\n",
    "\t\t\t\tparameters[\"b\"+str(l+1)]=np.zeros((dims[l+1],1))\n",
    "\n",
    "\t\t\telif(init_type==\"Xavier\"):\n",
    "\t\t\t\tparameters[\"W\"+str(l+1)]=np.random.randn(dims[l+1],dims[l])*np.sqrt(1/dims[l])\n",
    "\t\t\t\tparameters[\"b\"+str(l+1)]=np.zeros((dims[i+1],1))\n",
    "\t\t\t\n",
    "\t\treturn parameters\n",
    "\n",
    "\tdef forward_propagate(self,X,Y):\n",
    "\t\t''' X: Training Data with examples in column \n",
    "\t\t\tY: Training label with examples in column\n",
    "\t\t'''\n",
    "\n",
    "\t\tcache={}\n",
    "\t\tnum_layers=self.layers\n",
    "\t\tparameters=self.parameters\n",
    "\t\tactivation=self.activation_tup\n",
    "\t\tcache[\"A\"+str(0)]=X\n",
    "\n",
    "\t\t#Looping through the layer computing the values.\n",
    "\t\tfor l in range(num_layers):\n",
    "\t\t\tcache[\"Z\"+str(l+1)]=np.dot(parameters[\"W\"+str(l+1)],cache[\"A\"+str(l)])+parameters[\"b\"+str(l+1)]\n",
    "\t\t\tif(activation[l]==\"relu\"):\n",
    "\t\t\t\tcache[\"A\"+str(l+1)]=relu(cache[\"Z\"+str(l+1)])\n",
    "\t\t\t\n",
    "\t\t\telif(activation[l]==\"sigmoid\"):\n",
    "\t\t\t\tcache[\"A\"+str(l+1)]=sigmoid(cache[\"Z\"+str(l+1)])\n",
    "\t\tself.cache=cache\n",
    "\n",
    "\tdef calculate_cost(self,Y):\n",
    "\t\tbatch_size=Y.shape[1]\n",
    "\t\tlayers=self.layers\n",
    "\t\tAL=self.cache[\"A\"+str(layers)]\n",
    "\n",
    "\t\tlog_probs=Y*np.log(AL)+(1-Y)*np.log(1-AL)\n",
    "\t\tcost=(-1/batch_size)*np.squeeze(np.sum(log_probs))\n",
    "\n",
    "\t\treturn cost\n",
    "\n",
    "\tdef back_propagate_layer(self,m,layer):\n",
    "\t\t'''INPUT:  m: is batch size\n",
    "\t\t\t   layer: is exact layer number\n",
    "\t\t'''\n",
    "\t\tactivation=self.activation_tup[layer-1] #len(activation_tup) is total layer number-1\n",
    "\n",
    "\t\tdA=self.grads[\"dA\"+str(layer)]\n",
    "\t\tZ=self.cache[\"Z\"+str(layer)]\n",
    "\t\tA_prev=self.cache[\"A\"+str(layer-1)]\n",
    "\t\tW=self.parameters[\"W\"+str(layer)]\n",
    "\n",
    "\t\t#Activation Backpropagate\n",
    "\t\tif(activation==\"sigmoid\"):\n",
    "\t\t\tdZ=sigmoid_backward(dA,Z)\n",
    "\t\telif(activation==\"relu\"):\n",
    "\t\t\tdZ=relu_backward(dA,Z)\n",
    "\t\tself.grads[\"dZ\"+str(layer)]=dZ\n",
    "\n",
    "\t\t#Linear Backpropagate\n",
    "\t\tself.grads[\"dW\"+str(layer)]=(1/m)*np.dot(dZ,A_prev.T)\n",
    "\t\tself.grads[\"db\"+str(layer)]=(1/m)*np.sum(dZ,axis=1,keepdims=1)\n",
    "\n",
    "\t\tself.grads[\"dA\"+str(layer-1)]=np.dot(W.T,dZ)\n",
    "\n",
    "\tdef back_propagate_model(self,Y):\n",
    "\t\t''' INPUT: Y: Training Label\n",
    "\t\t\tOUTPUT: updates the model varible grads\n",
    "\t\t'''\n",
    "\t\tlayers=self.layers\n",
    "\t\tm=Y.shape[1]\n",
    "\n",
    "\t\tAL=self.cache[\"A\"+str(layers)]\n",
    "\t\tself.grads[\"dA\"+str(layers)]=np.divide(-1*Y,AL)+np.divide(1-Y,1-AL)\n",
    "\n",
    "\t\tfor l in reversed(range(1,layers+1)):\n",
    "\t\t\tself.back_propagate_layer(m,l)\n",
    "\t\t\t#print (self.grads[\"dW\"+str(l)].shape,self.grads[\"db\"+str(l)].shape)\n",
    "\n",
    "\tdef param_to_vector(self):\n",
    "\n",
    "\t\tparams=[]\n",
    "\t\tlayers=self.layers\n",
    "\t\tfor l in range(layers):\n",
    "\t\t\tparams.append(self.parameters[\"W\"+str(l+1)])\n",
    "\t\t\tparams.append(self.parameters[\"b\"+str(l+1)])\n",
    "\t\t#print (params)\n",
    "\t\tcount=0\n",
    "\t\tfor param in params:\n",
    "\t\t\t#print (param)\n",
    "\t\t\tnew_vector=np.reshape(param,(-1,1))\n",
    "\t\t\tif(count==0):\n",
    "\t\t\t\ttheta=new_vector\n",
    "\t\t\telse:\n",
    "\t\t\t\ttheta=np.concatenate((theta,new_vector),axis=0)\n",
    "\t\t\tcount=count+1\n",
    "\t\treturn theta\n",
    "\n",
    "\tdef grads_to_vector(self):\n",
    "\t\t\n",
    "\t\t#We have to bring the theta and grad in same order to make sense\n",
    "\t\tgrads=[]\n",
    "\t\tlayers=self.layers\n",
    "\t\tfor l in range(layers):\n",
    "\t\t\tgrads.append(self.grads[\"dW\"+str(l+1)])\n",
    "\t\t\tgrads.append(self.grads[\"db\"+str(l+1)])\n",
    "\t\t#print(grads)\n",
    "\t\tcount=0\n",
    "\t\tfor grad in grads:\n",
    "\t\t\t#print (grad)\n",
    "\t\t\tnew_vector=np.reshape(grad,(-1,1))\n",
    "\t\t\tif(count==0):\n",
    "\t\t\t\tgradient=new_vector\n",
    "\t\t\telse:\n",
    "\t\t\t\tgradient=np.concatenate((gradient,new_vector),axis=0)\n",
    "\t\t\tcount=count+1\n",
    "\t\t\t\n",
    "\t\treturn gradient\n",
    "\n",
    "\tdef vector_to_param(self,theta):\n",
    "\n",
    "\t\tlayers=self.layers\n",
    "\t\tparameters={}\n",
    "\n",
    "\t\tstart=0\n",
    "\t\tfor l in range(1,layers+1):\n",
    "\t\t\tn1,n2=self.parameters[\"W\"+str(l)].shape\n",
    "\t\t\tend=start+n1*n2\n",
    "\t\t\tparameters[\"W\"+str(l)]=theta[start:end].reshape((n1,n2))\n",
    "\t\t\tstart=end\n",
    "\t\t\tend=start+n1\n",
    "\t\t\tparameters[\"b\"+str(l)]=theta[start:end].reshape((n1,1))\n",
    "\t\t\tstart=end\n",
    "\t\treturn parameters\n",
    "\n",
    "\tdef gradient_checking(self,epsilon,X,Y):\n",
    "\n",
    "\t\tparam_vector=self.param_to_vector()\n",
    "\t\tgradient_vector=self.grads_to_vector()\n",
    "\t\tnum_param=param_vector.shape[0]\n",
    "\t\tJ_plus=np.zeros((num_param,1))\n",
    "\t\tJ_minus=np.zeros((num_param,1))\n",
    "\t\tgrad_approx=np.zeros((num_param,1))\n",
    "\n",
    "\t\tfor i in range(num_param):\n",
    "\n",
    "\t\t\t#Calculating the right sided value for Central Difference\n",
    "\t\t\ttheta_plus=np.copy(param_vector) #fresh memory (new clone) so that we could change\n",
    "\t\t\ttheta_plus[i][0]=theta_plus[i][0]+epsilon\n",
    "\t\t\tself.parameters=self.vector_to_param(theta_plus)\n",
    "\t\t\tself.forward_propagate(X,Y)\n",
    "\t\t\tJ_plus[i][0]=self.calculate_cost(Y)\n",
    "\n",
    "\t\t\t#Calculating the left side value for Central Differnce\n",
    "\t\t\ttheta_minus=np.copy(param_vector)\n",
    "\t\t\ttheta_minus[i][0]=theta_minus[i][0]-epsilon\n",
    "\t\t\tself.parameters=self.vector_to_param(theta_minus)\n",
    "\t\t\tself.forward_propagate(X,Y)\n",
    "\t\t\tJ_minus[i][0]=self.calculate_cost(Y)\n",
    "\n",
    "\t\t\tgrad_approx[i][0]=(J_plus[i][0]-J_minus[i][0])/(2*epsilon)\n",
    "\n",
    "\t\tnumerator=np.linalg.norm(gradient_vector-grad_approx)\n",
    "\t\tdenominator=np.linalg.norm(gradient_vector)+np.linalg.norm(grad_approx)\n",
    "\n",
    "\t\tdifference=numerator/denominator\n",
    "\t\tprint (\"difference in grad\",difference)\n",
    "\n",
    "\t\t#Re-assigning the initial parameters as model parameters\n",
    "\t\tself.parameters=self.vector_to_param(param_vector)\n",
    "\n",
    "\tdef update_parameter(self,learningRate):\n",
    "\t\t'''\n",
    "\t\tUpdating the parameter according to the gradient \n",
    "\t\tcalculated above.\n",
    "\t\t'''\n",
    "\t\tlayers=self.layers\n",
    "\t\tparameters=self.parameters\n",
    "\t\tgrads=self.grads\n",
    "\n",
    "\t\tfor l in range(1,layers+1):\n",
    "\t\t\tparameters[\"W\"+str(l)]=parameters[\"W\"+str(l)]-learningRate*grads[\"dW\"+str(l)]\n",
    "\t\t\tparameters[\"b\"+str(l)]=parameters[\"b\"+str(l)]-learningRate*grads[\"db\"+str(l)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 1)\n",
      "(1, 2)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "net=model(2,[3,2,1],\"HE\",(\"relu\",\"sigmoid\"))\n",
    "print (net.parameters[\"W1\"].shape)\n",
    "print (net.parameters[\"b1\"].shape)\n",
    "print (net.parameters[\"W2\"].shape)\n",
    "print (net.parameters[\"b2\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3190391  -0.24937038  1.46210794]\n",
      " [-2.06014071 -0.3224172  -0.38405435]\n",
      " [ 1.13376944 -1.09989127 -0.17242821]]\n",
      "[[ True  True False]]\n"
     ]
    }
   ],
   "source": [
    "X=np.random.randn(3,3)\n",
    "Y=np.random.randn(1,3)\n",
    "Y=Y<0.5\n",
    "print (X)\n",
    "print (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.forward_propagate(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A0': array([[ 0.3190391 , -0.24937038,  1.46210794],\n",
       "        [-2.06014071, -0.3224172 , -0.38405435],\n",
       "        [ 1.13376944, -1.09989127, -0.17242821]]),\n",
       " 'A1': array([[ 0.96322835,  0.30464196,  2.2053472 ],\n",
       "        [ 0.        ,  2.05756044,  0.        ]]),\n",
       " 'A2': array([[ 0.84299087,  0.26217657,  0.97912109]]),\n",
       " 'Z1': array([[ 0.96322835,  0.30464196,  2.2053472 ],\n",
       "        [-3.86578036,  2.05756044, -1.22826341]]),\n",
       " 'Z2': array([[ 1.68065215, -1.03468633,  3.84791574]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "param=net.parameters\n",
    "Z1=np.dot(param[\"W1\"],X)+param[\"b1\"]\n",
    "print (Z1==net.cache[\"Z1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "A1=relu(Z1)\n",
    "print (A1==net.cache[\"A1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "Z2=np.dot(param[\"W2\"],A1)+param[\"b2\"]\n",
    "print (Z2==net.cache[\"Z2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "A2=sigmoid(Z2)\n",
    "print (A2==net.cache[\"A2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7928506394467114"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.calculate_cost(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7928506394467114"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log=Y*np.log(A2)+(1-Y)*np.log(1-A2)\n",
    "np.sum(log)*(-1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.back_propagate_model(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param[\"W2\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param[\"b2\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param[\"W1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param[\"b1\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.10061918  1.14472371  0.90159072]\n",
      " [ 0.50249434  0.90085595 -0.68372786]]\n"
     ]
    }
   ],
   "source": [
    "a=np.random.randn(2,3)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.10061918]\n",
      " [ 1.14472371]\n",
      " [ 0.90159072]\n",
      " [ 0.50249434]\n",
      " [ 0.90085595]\n",
      " [-0.68372786]]\n"
     ]
    }
   ],
   "source": [
    "b=a.reshape(-1,1)\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.10061918,  1.14472371,  0.90159072],\n",
       "       [ 0.50249434,  0.90085595, -0.68372786]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.reshape(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict={1,2,3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in dict:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 1.32627244, -0.49949702, -0.43125043],\n",
       "        [-0.87607521,  0.70660237, -1.87919848]]),\n",
       " 'W2': array([[ 1.74481176, -0.7612069 ]]),\n",
       " 'b1': array([[ 0.],\n",
       "        [ 0.]]),\n",
       " 'b2': array([[ 0.]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.32627244]\n",
      " [-0.49949702]\n",
      " [-0.43125043]\n",
      " [-0.87607521]\n",
      " [ 0.70660237]\n",
      " [-1.87919848]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 1.74481176]\n",
      " [-0.7612069 ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "theta=net.param_to_vector()\n",
    "print (theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 1.32627244, -0.49949702, -0.43125043],\n",
      "       [-0.87607521,  0.70660237, -1.87919848]]), 'b1': array([[ 0.],\n",
      "       [ 0.]]), 'W2': array([[ 1.74481176, -0.7612069 ]]), 'b2': array([[ 0.]])}\n"
     ]
    }
   ],
   "source": [
    "print (net.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params=net.vector_to_param(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 1.32627244, -0.49949702, -0.43125043],\n",
      "       [-0.87607521,  0.70660237, -1.87919848]]), 'b1': array([[ 0.],\n",
      "       [ 0.]]), 'W2': array([[ 1.74481176, -0.7612069 ]]), 'b2': array([[ 0.]])}\n"
     ]
    }
   ],
   "source": [
    "print (params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.91048929  0.10777828  0.27026279]\n",
      " [-0.04668515 -0.0603604  -0.20591295]]\n",
      "[[ 0.04902254]\n",
      " [ 0.1872121 ]]\n",
      "[[ 0.59443144 -0.50603877]]\n",
      "[[ 0.02809618]]\n"
     ]
    }
   ],
   "source": [
    "print(net.grads[\"dW1\"])\n",
    "print(net.grads[\"db1\"])\n",
    "print(net.grads[\"dW2\"])\n",
    "print(net.grads[\"db2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference in grad 4.73352029244e-09\n"
     ]
    }
   ],
   "source": [
    "epsilon=1e-7\n",
    "net.gradient_checking(epsilon,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.append(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
